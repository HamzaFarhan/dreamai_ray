# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/index/02_core.ipynb.

# %% auto 0
__all__ = ['write_index_cb', 'reset_index_cb', 'IndexCreator', 'create_indexes_setup', 'create_indexes_iter', 'create_index_',
           'create_indexes_combine', 'create_indexes', 'search_indexes_setup', 'search_indexes_iter', 'search_index_',
           'search_indexes_combine', 'search_indexes']

# %% ../../nbs/index/02_core.ipynb 2
from ..imports import *
from ..utils import *
from ..mapper import *
from .utils import *
from .df import *


# %% ../../nbs/index/02_core.ipynb 4
class write_index_cb(Callback):
    "A `Callback` to write the index to disk."

    def after_batch(self, cls, *args, **kwargs):
        cls.index = cls.udf_kwargs["index"]
        index_folder = cls.index_folder
        os.makedirs(index_folder, exist_ok=True)
        index_path = str(Path(index_folder) / f"{cls.block_counter}_{cls.index.ntotal}.faiss")
        df_path = str(Path(index_folder) / f"{cls.block_counter}.csv")
        if self.verbose and cls.verbose:
            msg.info(f"Writing Index to {index_path}")
            msg.info(f"Index Size: {cls.index.ntotal}")
            msg.info(f"Writing DF to {df_path}")
        faiss.write_index(cls.index, index_path)
        kwargs["df"].reset_index(drop=True).to_csv(df_path, index=False)


class reset_index_cb(Callback):
    "A `Callback` to reset the index."

    def after_batch(self, cls, **kwargs):
        cls.index.reset()
        if self.verbose and cls.verbose:
            msg.info(f"Index Size Post Reset: {cls.index.ntotal}")
        cls.udf_kwargs["index"] = cls.index
        cls.udf = partial(cls.udf, **cls.udf_kwargs)


class IndexCreator(Mapper):
    "Creates indexes from embeddings."

    def __init__(
        self,
        index=None,
        index_dim=3,  # The dimension of the index.
        index_folder="indexes",  # The folder to write the index to.
        ems_col="embedding",  # The column to use to create the index.
        udf=df_to_index,  # The function to use to create the index.
        cbs=[write_index_cb, reset_index_cb],  # The `Callback`s to use.
        block_counter=0,  # The starting block counter.
        verbose=True,  # Whether to print out information.
        udf_verbose=False,  # Whether to print out information in the udf.
        udf_kwargs={},  # Additional kwargs to pass to the udf.
        **kwargs,
    ):
        self.index_folder = index_folder
        if index is None:
            self.index = create_index(index_dim)
        else:
            self.index = index
        udf_kwargs["index"] = self.index
        udf_kwargs["ems_col"] = ems_col
        udf_kwargs["verbose"] = udf_verbose
        self.verbose = verbose
        cbs = [block_counter_cb(block_counter)] + cbs
        super().__init__(**locals_to_params(locals()))


def create_indexes_setup(
    task_id,
    ems_folder,
    index_folder="indexes",
    local_index_folder="/media/hamza/data2/faiss_data/saved_indexes",
    ems_col="embedding",
    num_blocks=1,
    block_size=40000,
    block_counter=0,
    verbose=True,
    *args,
    **kwargs,
):
    task_folder = f"/tmp/{task_id}"
    t1 = time()
    ems_folder, _ = handle_input_path(ems_folder, local_path=task_folder)
    em_files = sorted(
        get_files(ems_folder, extensions=[".json"], make_str=True),
        key=lambda x: int(Path(x).stem.split("_")[-1]),
    )

    t2 = time()
    msg.info(
        f"Embeddings download time: {t2-t1:.2f} seconds.",
        spaced=True,
        show=verbose,
    )
    df = pd.DataFrame({ems_col: em_files})
    msg.info(f"Embeddings DF created of length: {len(df)}", spaced=True, show=verbose)

    index_folder, index_bucket = get_local_path(index_folder, local_path=task_folder)
    local_index_folder = Path(local_index_folder) / Path(index_folder).name
    if local_index_folder.exists():
        msg.info(f"Local Index Folder Exists: {local_index_folder}", spaced=True, show=verbose)
        bucket_size = len(get_files(local_index_folder, extensions=[".faiss"]))
    else:
        bucket_size = max(bucket_count(index_bucket), 0) // 2
    block_counter += bucket_size
    if num_blocks is not None:
        block_size = len(df) // num_blocks
    msg.info(f"Bucket Size: {bucket_size}", spaced=True, show=verbose)
    msg.info(f"Block Size: {block_size}", spaced=True, show=verbose)
    msg.info(f"Block Counter: {block_counter}", spaced=True, show=verbose)

    return (
        df,
        block_size,
        block_counter,
        task_folder,
        index_folder,
        index_bucket,
        local_index_folder,
    )


def create_indexes_iter(
    ems_folder="embeddings",  # The folder containing the embeddings.
    ems_col="embedding",  # The column to use to create the index.
    index_dim=768,  # The dimension of the index.
    index_folder="indexes",  # The folder to write the index to.
    local_index_folder="/media/hamza/data2/faiss_data/saved_indexes",  # The local folder to write the index to.
    num_blocks=1,  # The number of blocks to make.
    block_size=40000,  # The size of each block. Will be ignored if `num_blocks` is not None.
    block_counter=0,  # The starting block counter.
    verbose=True,  # Whether to print out information.
    udf_verbose=False,  # Whether to print out information in the udf.
    udf_kwargs={},  # Additional kwargs to pass to the udf.
    task_id=gen_random_string(16),  # The task id to use.
    *args,
    **kwargs,
):
    t1 = time()
    (
        df,
        block_size,
        block_counter,
        task_folder,
        index_folder,
        index_bucket,
        local_index_folder,
    ) = create_indexes_setup(
        task_id=task_id,
        ems_folder=ems_folder,
        index_folder=index_folder,
        local_index_folder=local_index_folder,
        ems_col=ems_col,
        num_blocks=num_blocks,
        verbose=verbose,
    )
    t2 = time()
    msg.info(f"Setup time: {t2-t1:.2f} seconds.", spaced=True, show=verbose)

    for i in range(0, len(df), block_size):
        cls_kwargs = dict(
            index_dim=index_dim,
            index_folder=index_folder,
            ems_col=ems_col,
            block_counter=block_counter,
            verbose=verbose,
            udf_verbose=udf_verbose,
            udf_kwargs=udf_kwargs,
        )
        fn_kwargs = dict(
            task_folder=task_folder,
            index_folder=index_folder,
            index_bucket=index_bucket,
            local_index_folder=local_index_folder,
        )
        block_counter += 1
        yield dict(df=df.iloc[i : i + block_size], fn_kwargs=fn_kwargs, cls_kwargs=cls_kwargs)


def create_index_(
    data_dict,
    **kwargs,
):
    df = data_dict["df"]
    cls_kwargs = data_dict["cls_kwargs"]
    m = IndexCreator(**cls_kwargs)
    m(df)
    return data_dict["fn_kwargs"]


def create_indexes_combine(res_list, *args, **kwargs):
    task_folder = res_list[0]["task_folder"]
    index_folder = res_list[0]["index_folder"]
    index_bucket = res_list[0]["index_bucket"]
    local_index_folder = res_list[0]["local_index_folder"]
    shutil.copytree(index_folder, local_index_folder, dirs_exist_ok=True)
    bucket_up(index_folder, index_bucket, only_new=True)
    shutil.rmtree(task_folder, ignore_errors=True)
    return {"index_folder": str(index_bucket), "local_index_folder": str(local_index_folder)}


def create_indexes(
    ems_folder="embeddings",  # The folder containing the embeddings.
    ems_col="embedding",  # The column to use to create the index.
    index_dim=768,  # The dimension of the index.
    index_folder="indexes",  # The folder to write the index to.
    local_index_folder="/media/hamza/data2/faiss_data/saved_indexes",  # The local folder to write the index to.
    num_blocks=1,  # The number of blocks to make.
    block_size=40000,  # The size of each block. Will be ignored if `num_blocks` is not None.
    block_counter=0,  # The starting block counter.
    verbose=True,  # Whether to print out information.
    udf_verbose=False,  # Whether to print out information in the udf.
    udf_kwargs={},  # Additional kwargs to pass to the udf.
    task_id=gen_random_string(16),  # The task id to use.
    *args,
    **kwargs,
):
    "Function to create indexes from embeddings."
    t_1 = time()
    try:
        t1 = time()
        iterator = create_indexes_iter(**locals_to_params(locals()))
        t2 = time()
        msg.info(f"Iterator creation time: {t2-t1:.2f} seconds.", spaced=True, show=verbose)
    except Exception as e:
        msg.fail(f"Error creating iterator: {e}", spaced=True, show=verbose)
        raise e

    try:
        t1 = time()
        create_res = [create_index_(data_dict) for data_dict in iterator]
        t2 = time()
        msg.info(f"Index creation time: {t2-t1:.2f} seconds.", spaced=True, show=verbose)
    except Exception as e:
        msg.fail(f"Error creating indexes: {e}", spaced=True, show=verbose)
        raise e

    try:
        t1 = time()
        combine_res = create_indexes_combine(create_res)
        t2 = time()
        msg.info(f"Index combination time: {t2-t1:.2f} seconds.", spaced=True, show=verbose)
    except Exception as e:
        msg.fail(f"Error combining indexes: {e}", spaced=True, show=verbose)
        raise e
    t_2 = time()
    msg.good(f"Total creation time: {t_2-t_1:.2f} seconds.", spaced=True, show=verbose)
    return combine_res


def search_indexes_setup(
    task_id,
    ems,
    index_folder,
    local_index_folder="/media/hamza/data2/faiss_data/saved_indexes",
    verbose=True,
):
    task_folder = f"/tmp/{task_id}"
    # if local_index_folder is None:
    #     index_folder, _ = handle_input_path(
    #         index_folder, local_path=local_index_folder, task_id=task_id
    #     )
    # else:
    pre_index_folder, _ = get_local_path(index_folder, local_path=local_index_folder)
    if os.path.exists(pre_index_folder):
        msg.info(f"Cached Index Folder: {pre_index_folder}", spaced=True, show=verbose)
        index_folder = pre_index_folder
    else:
        index_folder, _ = handle_input_path(
            index_folder, local_path=local_index_folder, task_id=task_id
        )
    bucket_dl(ems, task_folder)
    ems_file = get_files(task_folder, extensions=[".json"])[0]
    with open(ems_file) as f:
        ems = json.load(f)["embedding"]
    indexes = sorted(
        get_files(index_folder, extensions=[".faiss"]),
        key=lambda x: int(x.stem.split("_")[0]),
    )
    if not os.path.exists(index_folder) or len(indexes) == 0:
        raise Exception(
            f"No indexes found in '{index_folder}' folder. Please create indexes first."
        )
    indexes = sorted(
        get_files(index_folder, extensions=[".faiss"]),
        key=lambda x: int(x.stem.split("_")[0]),
    )
    return ems, indexes, index_folder, task_folder


def search_indexes_iter(
    ems,  # The embedding to search. Can be pre-loaded or a path to a json file.
    index_folder,  # The remote folder containing the indexes.
    local_index_folder="/media/hamza/data2/faiss_data/saved_indexes",  # Not required if `index_folder` is local.
    k=2,  # The number of nearest neighbors to return.
    verbose=True,  # Whether to print out information.
    task_id=gen_random_string(16),  # The task id to use.
    *args,
    **kwargs,
):
    ems, indexes, index_folder, task_folder = search_indexes_setup(
        task_id, ems, index_folder, local_index_folder, verbose
    )
    for index in indexes:
        yield {
            "qdf": pd.DataFrame({"index": [index], "embedding": [ems]}),
            "k": k,
            "index_folder": index_folder,
            "task_folder": task_folder,
            "verbose": verbose,
        }


def search_index_(data_dict):
    qdf = data_dict["qdf"]
    k = data_dict["k"]
    verbose = data_dict["verbose"]
    qdf = qdf.apply(lambda x: df_index_search(x, k=k, verbose=verbose), axis=1)
    data_dict["qdf"] = qdf
    return data_dict


def search_indexes_combine(res_list, *args, **kwargs):
    r0 = res_list[0]
    k = r0["k"]
    verbose = r0["verbose"]
    index_folder = r0["index_folder"]
    task_folder = r0["task_folder"]
    qdf = pd.concat([d["qdf"] for d in res_list]).reset_index(drop=True)
    res = index_heap(qdf, k=k, verbose=verbose, with_offset=True)
    dfs = sorted(get_files(index_folder, extensions=[".csv"]), key=lambda x: int(x.stem))
    df = pd.concat([pd.read_csv(df) for df in dfs]).reset_index(drop=True)
    res["meta_data"] = df.iloc[res["ids"][0]].to_dict(orient="records")
    shutil.rmtree(task_folder)
    return res


def search_indexes(
    ems,  # The embedding to search. Can be pre-loaded or a path to a json file.
    index_folder,  # The remote folder containing the indexes.
    local_index_folder="/media/hamza/data2/faiss_data/saved_indexes",  # Not required if `index_folder` is local.
    k=2,  # The number of nearest neighbors to return.
    verbose=True,  # Whether to print out information.
    task_id=gen_random_string(16),  # The task id to use.
):
    "Function to search an embedding against indexes."

    t_1 = time()
    try:
        t1 = time()
        iterator = search_indexes_iter(**locals_to_params(locals()))
        t2 = time()
        msg.info(f"Iterator creation time: {t2-t1:.2f} seconds.", spaced=True, show=verbose)
    except Exception as e:
        msg.fail(f"Error creating iterator: {e}", spaced=True, show=verbose)
        raise e

    try:
        t1 = time()
        create_res = [search_index_(data_dict) for data_dict in iterator]
        t2 = time()
        msg.info(f"Index creation time: {t2-t1:.2f} seconds.", spaced=True, show=verbose)
    except Exception as e:
        msg.fail(f"Error creating indexes: {e}", spaced=True, show=verbose)
        raise e

    try:
        t1 = time()
        combine_res = search_indexes_combine(create_res)
        t2 = time()
        msg.info(f"Search combination time: {t2-t1:.2f} seconds.", spaced=True, show=verbose)
    except Exception as e:
        msg.fail(f"Error combining searches: {e}", spaced=True, show=verbose)
        raise e
    t_2 = time()
    msg.good(f"Total search time: {t_2-t_1:.2f} seconds.", spaced=True, show=verbose)
    return combine_res
