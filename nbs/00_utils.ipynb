{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Utils for the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from dreamai.core import *\n",
    "from dreamai_ray.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def json_file(path, folder):\n",
    "    path = Path(path)\n",
    "    folder = Path(folder)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    return folder / f\"{path.stem}.json\"\n",
    "\n",
    "\n",
    "def get_task_from_kv_store(task_id, kv_store):\n",
    "    task = kv_store.get(task_id)\n",
    "    if task is None:\n",
    "        raise Exception(f\"No task entry found for task_id {task_id}.\")\n",
    "    if type(task) != dict:\n",
    "        raise Exception(f\"Wrong type for task with task_id {task_id}.\")\n",
    "    if len(task) == 0:\n",
    "        raise Exception(f\"Empty dict for task_id {task_id}.\")\n",
    "    return task\n",
    "\n",
    "\n",
    "def init_task_progress(task_id, kv_store, total):\n",
    "    task = get_task_from_kv_store(task_id, kv_store)\n",
    "    task[\"progress\"] = f\"processing...\"\n",
    "    task[\"total\"] = total\n",
    "    kv_store.insert(task_id, task)\n",
    "\n",
    "\n",
    "def update_task_progress(task_id, kv_store, **kwargs):\n",
    "    task = get_task_from_kv_store(task_id, kv_store)\n",
    "    prog = task[\"progress\"]\n",
    "    total = task[\"total\"]\n",
    "    if prog == \"processing...\":\n",
    "        task[\"progress\"] = f\"1/{total}\"\n",
    "    else:\n",
    "        curr, total = prog.split(\"/\")\n",
    "        task[\"progress\"] = f\"{int(curr) + 1}/{total}\"\n",
    "    kv_store.insert(task_id, task)\n",
    "\n",
    "\n",
    "def json_file(path, folder):\n",
    "    path = Path(path)\n",
    "    folder = Path(folder)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    return folder / f\"{path.stem}.json\"\n",
    "\n",
    "\n",
    "def is_bucket(p):\n",
    "    return str(p).startswith(\"gs://\")\n",
    "\n",
    "\n",
    "def gsutil_bucket(bucket):\n",
    "    if not str(bucket).startswith(\"gs://\"):\n",
    "        bucket = \"gs://\" + str(bucket)\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def gsutil_src(folder):\n",
    "    if Path(folder).suffix != \"\":\n",
    "        return str(folder)\n",
    "    folder = str(folder)\n",
    "    if folder[-1] != \"/\":\n",
    "        folder += \"/\"\n",
    "    folder += \"*\"\n",
    "    return folder\n",
    "\n",
    "\n",
    "def bucket_move(folder, bucket):\n",
    "    gu = shutil.which(\"gsutil\")\n",
    "    bucket = gsutil_bucket(bucket)\n",
    "    folder = gsutil_src(folder)\n",
    "    subprocess.run([gu, \"-m\", \"mv\", folder, bucket])\n",
    "\n",
    "\n",
    "def bucket_up(folder, bucket, only_new=True):\n",
    "    gu = shutil.which(\"gsutil\")\n",
    "    bucket = gsutil_bucket(bucket)\n",
    "    folder = gsutil_src(folder)\n",
    "    cmd = [gu, \"-m\", \"cp\", \"-r\"]\n",
    "    if only_new:\n",
    "        cmd.append(\"-n\")\n",
    "    subprocess.run(cmd + [folder, bucket])\n",
    "\n",
    "\n",
    "def bucket_dl(bucket, folder, only_new=True):\n",
    "    gu = shutil.which(\"gsutil\")\n",
    "    bucket = gsutil_bucket(bucket)\n",
    "    bucket = gsutil_src(bucket)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    cmd = [gu, \"-m\", \"cp\", \"-r\"]\n",
    "    if only_new:\n",
    "        cmd.append(\"-n\")\n",
    "    subprocess.run(cmd + [bucket, folder])\n",
    "\n",
    "\n",
    "def get_local_path(folder, task_folder):\n",
    "    if is_bucket(folder):\n",
    "        if Path(folder).suffix != \"\":\n",
    "            return Path(task_folder)\n",
    "        return Path(task_folder) / Path(folder).name\n",
    "    else:\n",
    "        return Path(folder)\n",
    "\n",
    "\n",
    "def lit_eval(x):\n",
    "    try:\n",
    "        return literal_eval(x)\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "\n",
    "def find_alternate_path(path):\n",
    "    path = Path(path)\n",
    "    idx = 0\n",
    "    file_start = \"/\".join(path.parts[:-1])\n",
    "    if file_start[:2] == \"//\":\n",
    "        file_start = file_start[1:]\n",
    "    file_start = Path(file_start)\n",
    "    file_end = path.stem\n",
    "    new_path = file_start / f\"{file_end}{path.suffix}\"\n",
    "    while new_path.exists():\n",
    "        new_path = file_start / f\"{file_end}_{idx}{path.suffix}\"\n",
    "        idx += 1\n",
    "    msg.info(f\"{path} already exists. Using {new_path} instead.\", spaced=True)\n",
    "    return new_path\n",
    "\n",
    "\n",
    "def resolve_ds_path(ds_path, append=False, overwrite=False):\n",
    "    ds_path = Path(ds_path)\n",
    "    if ds_path.is_dir():\n",
    "        if append:\n",
    "            msg.info(f\"{ds_path} already exists. Appending because append=True.\", spaced=True)\n",
    "            return ds_path\n",
    "        elif overwrite:\n",
    "            msg.info(\n",
    "                f\"\\n{ds_path} already exists. Overwriting because overwrite=True.\\n\",\n",
    "                spaced=True,\n",
    "            )\n",
    "            shutil.rmtree(ds_path)\n",
    "            return ds_path\n",
    "        ds_path = find_alternate_path(ds_path)\n",
    "    return ds_path\n",
    "\n",
    "\n",
    "def write_ds(ds, ds_path, append=False, overwrite=False, **kwargs):\n",
    "    ds_path = resolve_ds_path(ds_path, append, overwrite=overwrite)\n",
    "    ds.write_parquet(ds_path, **kwargs)\n",
    "    return ds_path\n",
    "\n",
    "\n",
    "def chain_models(models):\n",
    "    if not is_list(models):\n",
    "        models = [models]\n",
    "    return nn.Sequential(*models)\n",
    "\n",
    "\n",
    "def is_preprocessor(x):\n",
    "    return isinstance(x, rd.Preprocessor)\n",
    "\n",
    "\n",
    "def chain_processors(processors):\n",
    "    if not is_list(processors):\n",
    "        processors = [processors]\n",
    "    return Chain(*processors)\n",
    "\n",
    "\n",
    "def handle_processors(processors, batch_size=None):\n",
    "    if processors is None:\n",
    "        return None\n",
    "    if not is_list(processors):\n",
    "        processors = [processors]\n",
    "    if len(processors) == 0:\n",
    "        return None\n",
    "\n",
    "    def to_bm(p, bs):\n",
    "        if not is_preprocessor(p) and callable(p):\n",
    "            return BatchMapper(p, batch_size=bs, batch_format=\"pandas\")\n",
    "        elif is_preprocessor(p):\n",
    "            return p\n",
    "\n",
    "    return chain_processors([to_bm(p, batch_size) for p in processors if p is not None])\n",
    "\n",
    "\n",
    "def repartition_ds(ds, num_blocks=2):\n",
    "    if path_or_str(ds):\n",
    "        ds = rd.read_parquet(ds)\n",
    "    try:\n",
    "        if num_blocks is not None and num_blocks > 0 and ds.num_blocks() != num_blocks:\n",
    "            ds = ds.repartition(num_blocks)\n",
    "    except:\n",
    "        pass\n",
    "    return ds\n",
    "\n",
    "\n",
    "def transform_ds(ds, processors=[], num_blocks=2, batch_size=32, **kwargs):\n",
    "    ds = repartition_ds(ds, num_blocks=num_blocks)\n",
    "    pp = handle_processors(processors, batch_size=batch_size)\n",
    "    if pp is None:\n",
    "        return ds\n",
    "    return pp.transform(ds)\n",
    "\n",
    "\n",
    "def group_df_on(df, group_on=\"path\", agg_on=[\"text\"]):\n",
    "    if not is_list(agg_on):\n",
    "        agg_on = [agg_on]\n",
    "    agg_dict = {k: lambda x: list(x) for k in agg_on}\n",
    "    return df.groupby(group_on, as_index=False).agg(agg_dict).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
