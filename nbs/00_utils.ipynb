{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Utils for the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from dreamai.core import *\n",
    "from dreamai_ray.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def gen_random_string(length):\n",
    "    return str(uuid.uuid4()).replace(\"-\", \"\")[:length]\n",
    "\n",
    "\n",
    "def json_file(path, folder):\n",
    "    path = Path(path)\n",
    "    folder = Path(folder)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    return folder / f\"{path.stem}.json\"\n",
    "\n",
    "\n",
    "def get_task_from_kv_store(task_id, kv_store):\n",
    "    task = kv_store.get(task_id)\n",
    "    if task is None:\n",
    "        raise Exception(f\"No task entry found for task_id {task_id}.\")\n",
    "    if type(task) != dict:\n",
    "        raise Exception(f\"Wrong type for task with task_id {task_id}.\")\n",
    "    if len(task) == 0:\n",
    "        raise Exception(f\"Empty dict for task_id {task_id}.\")\n",
    "    return task\n",
    "\n",
    "\n",
    "def init_task_progress(task_id, kv_store, total):\n",
    "    task = get_task_from_kv_store(task_id, kv_store)\n",
    "    task[\"progress\"] = f\"processing...\"\n",
    "    task[\"total\"] = total\n",
    "    kv_store.insert(task_id, task)\n",
    "\n",
    "\n",
    "def update_task_progress(task_id, kv_store, **kwargs):\n",
    "    task = get_task_from_kv_store(task_id, kv_store)\n",
    "    prog = task[\"progress\"]\n",
    "    total = task[\"total\"]\n",
    "    if prog == \"processing...\":\n",
    "        task[\"progress\"] = f\"1/{total}\"\n",
    "    else:\n",
    "        curr, total = prog.split(\"/\")\n",
    "        task[\"progress\"] = f\"{int(curr) + 1}/{total}\"\n",
    "    kv_store.insert(task_id, task)\n",
    "\n",
    "\n",
    "def json_file(path, folder):\n",
    "    path = Path(path)\n",
    "    folder = Path(folder)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    return folder / f\"{path.stem}.json\"\n",
    "\n",
    "\n",
    "def is_bucket(p):\n",
    "    return str(p).startswith(\"gs://\")\n",
    "\n",
    "\n",
    "def gsutil_bucket(bucket):\n",
    "    if not str(bucket).startswith(\"gs://\"):\n",
    "        bucket = \"gs://\" + str(bucket)\n",
    "    if bucket[-1] != \"/\" and Path(bucket).suffix != \".json\":\n",
    "        bucket += \"/\"\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def gsutil_src(folder):\n",
    "    if Path(folder).suffix != \"\":\n",
    "        return str(folder)\n",
    "    folder = str(folder)\n",
    "    if folder[-1] != \"/\":\n",
    "        folder += \"/\"\n",
    "    folder += \"*\"\n",
    "    return folder\n",
    "\n",
    "\n",
    "def bucket_del(bucket):\n",
    "    msg.info(f\"Deleting {bucket}.\", spaced=True)\n",
    "    gu = shutil.which(\"gsutil\")\n",
    "    bucket = gsutil_bucket(bucket)\n",
    "    subprocess.run([gu, \"-m\", \"rm\", \"-r\", bucket])\n",
    "\n",
    "def bucket_count(bucket):\n",
    "    gu = shutil.which(\"gsutil\")\n",
    "    bucket = gsutil_src(bucket)\n",
    "    cmd = [gu, \"ls\", \"-l\", bucket]\n",
    "    res = subprocess.run(cmd, stdout=subprocess.PIPE)\n",
    "    res = res.stdout.decode(\"utf-8\").split(\"\\n\")\n",
    "    return len(res) - 2\n",
    "\n",
    "\n",
    "def bucket_move(folder, bucket):\n",
    "    msg.info(f\"Moving {folder} to {bucket}.\", spaced=True)\n",
    "    gu = shutil.which(\"gsutil\")\n",
    "    bucket = gsutil_bucket(bucket)\n",
    "    folder = gsutil_src(folder)\n",
    "    files = get_files(folder, make_str=True)\n",
    "    if len(files) == 1:\n",
    "        folder = files[0]\n",
    "    subprocess.run([gu, \"-m\", \"mv\", folder, bucket])\n",
    "\n",
    "\n",
    "def bucket_up(folder, bucket, only_new=True):\n",
    "    msg.info(f\"Uploading {folder} to {bucket}.\", spaced=True)\n",
    "    gu = shutil.which(\"gsutil\")\n",
    "    bucket = gsutil_bucket(bucket)\n",
    "    folder = gsutil_src(folder)\n",
    "    cmd = [gu, \"-m\", \"cp\"]\n",
    "    if only_new:\n",
    "        cmd.append(\"-n\")\n",
    "    files = get_files(folder, make_str=True)\n",
    "    if len(files) == 1:\n",
    "        folder = files[0]\n",
    "    else:\n",
    "        cmd.append(\"-r\")\n",
    "    subprocess.run(cmd + [folder, bucket])\n",
    "\n",
    "\n",
    "def bucket_dl(bucket, folder, only_new=True):\n",
    "    msg.info(f\"Downloading {bucket} to {folder}.\", spaced=True)\n",
    "    gu = shutil.which(\"gsutil\")\n",
    "    bucket = gsutil_bucket(bucket)\n",
    "    bucket = gsutil_src(bucket)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    cmd = [gu, \"-m\", \"cp\", \"-r\"]\n",
    "    if only_new:\n",
    "        cmd.append(\"-n\")\n",
    "    subprocess.run(cmd + [bucket, folder])\n",
    "\n",
    "\n",
    "def get_local_path(remote_path, local_path):\n",
    "    if is_bucket(remote_path):\n",
    "        if Path(remote_path).suffix != \"\":\n",
    "            return Path(local_path), remote_path\n",
    "        return Path(local_path) / Path(remote_path).name, remote_path\n",
    "    else:\n",
    "        return Path(remote_path), remote_path\n",
    "\n",
    "\n",
    "def handle_input_path(\n",
    "    path, local_path=None, task_id=gen_random_string(16), only_new=True\n",
    "):\n",
    "    path = str(path)\n",
    "    if not is_bucket(path):\n",
    "        return path, path\n",
    "    if local_path is None:\n",
    "        local_path = Path(f\"/tmp/{task_id}\")\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "    local_path, bucket = get_local_path(path, local_path)\n",
    "    bucket_dl(bucket, local_path, only_new=only_new)\n",
    "    return local_path, bucket\n",
    "\n",
    "\n",
    "def lit_eval(x):\n",
    "    try:\n",
    "        return literal_eval(x)\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "\n",
    "def find_alternate_path(path):\n",
    "    path = Path(path)\n",
    "    idx = 0\n",
    "    file_start = \"/\".join(path.parts[:-1])\n",
    "    if file_start[:2] == \"//\":\n",
    "        file_start = file_start[1:]\n",
    "    file_start = Path(file_start)\n",
    "    file_end = path.stem\n",
    "    new_path = file_start / f\"{file_end}{path.suffix}\"\n",
    "    while new_path.exists():\n",
    "        new_path = file_start / f\"{file_end}_{idx}{path.suffix}\"\n",
    "        idx += 1\n",
    "    msg.info(f\"{path} already exists. Using {new_path} instead.\", spaced=True)\n",
    "    return new_path\n",
    "\n",
    "\n",
    "def resolve_ds_path(ds_path, append=False, overwrite=False):\n",
    "    ds_path = Path(ds_path)\n",
    "    if ds_path.is_dir():\n",
    "        if append:\n",
    "            msg.info(\n",
    "                f\"{ds_path} already exists. Appending because append=True.\", spaced=True\n",
    "            )\n",
    "            return ds_path\n",
    "        elif overwrite:\n",
    "            msg.info(\n",
    "                f\"\\n{ds_path} already exists. Overwriting because overwrite=True.\\n\",\n",
    "                spaced=True,\n",
    "            )\n",
    "            shutil.rmtree(ds_path)\n",
    "            return ds_path\n",
    "        ds_path = find_alternate_path(ds_path)\n",
    "    return ds_path\n",
    "\n",
    "\n",
    "def write_ds(ds, ds_path, append=False, overwrite=False, **kwargs):\n",
    "    ds_path = resolve_ds_path(ds_path, append, overwrite=overwrite)\n",
    "    ds.write_parquet(ds_path, **kwargs)\n",
    "    return ds_path\n",
    "\n",
    "\n",
    "def repartition_ds(ds, num_blocks=2):\n",
    "    if path_or_str(ds):\n",
    "        ds = rd.read_parquet(ds)\n",
    "    try:\n",
    "        if num_blocks is not None and num_blocks > 0 and ds.num_blocks() != num_blocks:\n",
    "            ds = ds.repartition(num_blocks)\n",
    "    except:\n",
    "        pass\n",
    "    return ds\n",
    "\n",
    "\n",
    "def group_df_on(df, group_on=\"path\", agg_on=[\"text\"]):\n",
    "    if not is_list(agg_on):\n",
    "        agg_on = [agg_on]\n",
    "    agg_dict = {k: lambda x: list(x) for k in agg_on}\n",
    "    return df.groupby(group_on, as_index=False).agg(agg_dict).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('67a1543689e64861',\n",
       " '/tmp/ca8f14a4fa68456c/ems_1',\n",
       " '/tmp/ca8f14a4fa68456c/ems_1')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "task_id = gen_random_string(16)\n",
    "ems_folder = \"gs://gcsfuse-talentnet-dev/ems_1\"\n",
    "ems_folder = \"/tmp/ca8f14a4fa68456c/ems_1\"\n",
    "local_ems_folder, ems_folder = handle_input_path(ems_folder, task_id=task_id)\n",
    "task_id, local_ems_folder, ems_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
