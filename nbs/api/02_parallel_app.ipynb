{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App\n",
    "\n",
    "> FastAPI parallel app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp api.parallel_app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from dreamai_ray.imports import *\n",
    "from dreamai_ray.utils import *\n",
    "from dreamai_ray.index.core import *\n",
    "from dreamai_ray.parallel.parallelizer import *\n",
    "from dreamai_ray.api.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "app = FastAPI(title=\"DreamAI Parallel API\", version=\"0.0.2\")\n",
    "\n",
    "\n",
    "@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 6})\n",
    "@serve.ingress(app)\n",
    "class DreamAIIndex:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actors=2,\n",
    "        num_cpus=1,\n",
    "        num_gpus=0,\n",
    "    ) -> None:\n",
    "        try:\n",
    "            self.num_actors = num_actors\n",
    "            self.num_cpus = num_cpus\n",
    "            self.num_gpus = num_gpus\n",
    "            self.index_pool_mapper = DataParallelizer.remote(\n",
    "                IndexCreatorPoolActor,\n",
    "                create_indexes_iter,\n",
    "                num_actors=num_actors,\n",
    "                num_cpus=num_cpus,\n",
    "                num_gpus=num_gpus,\n",
    "                combiner=create_indexes_combine,\n",
    "                verbose=True,\n",
    "            )\n",
    "            self.search_pool_mapper = DataParallelizer.remote(\n",
    "                SearchIndexPoolActor,\n",
    "                search_indexes_iter,\n",
    "                num_actors=num_actors,\n",
    "                num_cpus=num_cpus,\n",
    "                num_gpus=num_gpus,\n",
    "                combiner=search_indexes_combine,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            msg.fail(f\"Mappers creation failed with error {e}\", spaced=True)\n",
    "\n",
    "    async def index_action(self, data_dict: dict):\n",
    "        # t1 = time()\n",
    "        res = ray.get(self.index_pool_mapper.do_parallel.remote(data_dict=data_dict))\n",
    "        # t2 = time()\n",
    "        # msg.good(f\"Index Creation Time = {t2-t1:.2f}.\", spaced=True, show=True)\n",
    "        return res\n",
    "\n",
    "    async def search_action(self, data_dict: dict):\n",
    "        # t1 = time()\n",
    "        res = ray.get(self.search_pool_mapper.do_parallel.remote(data_dict=data_dict))\n",
    "        # t2 = time()\n",
    "        # msg.good(f\"Index Searching Time = {t2-t1:.2f}.\", spaced=True, show=True)\n",
    "        return res\n",
    "\n",
    "    @serve.batch(max_batch_size=5, batch_wait_timeout_s=0.2)\n",
    "    async def index_handle_batched(self, data_dict_list=None) -> list:\n",
    "        if data_dict_list is None:\n",
    "            raise Exception(f\"Data dict list is None.\")\n",
    "        msg.info(f\"BATCHES RECEIVED = {data_dict_list}\", spaced=True)\n",
    "        res = [self.index_action(data_dict) for data_dict in data_dict_list]\n",
    "        return res\n",
    "\n",
    "    @serve.batch(max_batch_size=5, batch_wait_timeout_s=0.2)\n",
    "    async def search_handle_batched(self, data_dict_list=None) -> list:\n",
    "        if data_dict_list is None:\n",
    "            raise Exception(f\"Data dict list is None.\")\n",
    "        msg.info(f\"BATCHES RECEIVED = {data_dict_list}\", spaced=True)\n",
    "        res = [self.search_action(data_dict) for data_dict in data_dict_list]\n",
    "        return res\n",
    "\n",
    "    @app.post(\"/index/create\")\n",
    "    async def create(self, index_data: IndexData):\n",
    "        data_dict = dict(\n",
    "            ems_folder=index_data.ems_folder, index_folder=index_data.index_folder\n",
    "        )\n",
    "        t1 = time()\n",
    "        res_ref = await self.index_handle_batched(data_dict)\n",
    "        res = await res_ref\n",
    "        t2 = time()\n",
    "        msg.good(f\"Index Creation Time = {t2-t1:.2f}.\", spaced=True)\n",
    "        return res\n",
    "\n",
    "    @app.post(\"/index/update\")\n",
    "    async def update(self, index_data: IndexData):\n",
    "        data_dict = dict(\n",
    "            ems_folder=index_data.ems_folder, index_folder=index_data.index_folder\n",
    "        )\n",
    "        t1 = time()\n",
    "        res_ref = await self.index_handle_batched(data_dict)\n",
    "        res = await res_ref\n",
    "        t2 = time()\n",
    "        msg.good(f\"Index Update Time = {t2-t1:.2f}.\", spaced=True)\n",
    "        return res\n",
    "\n",
    "    @app.post(\"/index/matching\")\n",
    "    async def match_ems(self, match_data: MatchData):\n",
    "        data_dict = dict(\n",
    "            ems=match_data.ems, index_folder=match_data.index_folder, k=match_data.k\n",
    "        )\n",
    "        t1 = time()\n",
    "        res_ref = await self.search_handle_batched(data_dict)\n",
    "        res = await res_ref\n",
    "        t2 = time()\n",
    "        msg.good(f\"Index Searching Time = {t2-t1:.2f}.\", spaced=True)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "dai_index = DreamAIIndex.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
