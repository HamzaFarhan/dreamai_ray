{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core functionality for indexing and searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp index.core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from dreamai_ray.imports import *\n",
    "from dreamai_ray.utils import *\n",
    "from dreamai_ray.mapper import *\n",
    "from dreamai_ray.index.utils import *\n",
    "from dreamai_ray.index.df import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class write_index_cb(Callback):\n",
    "    \"A `Callback` to write the index to disk.\"\n",
    "\n",
    "    def after_batch(self, cls, *args, **kwargs):\n",
    "        cls.index = cls.udf_kwargs[\"index\"]\n",
    "        index_folder = cls.index_folder\n",
    "        os.makedirs(index_folder, exist_ok=True)\n",
    "        index_path = str(Path(index_folder) / f\"{cls.block_counter}_{cls.index.ntotal}.faiss\")\n",
    "        df_path = str(Path(index_folder) / f\"{cls.block_counter}.csv\")\n",
    "        if self.verbose and cls.verbose:\n",
    "            msg.info(f\"Writing Index to {index_path}\")\n",
    "            msg.info(f\"Index Size: {cls.index.ntotal}\")\n",
    "            msg.info(f\"Writing DF to {df_path}\")\n",
    "        faiss.write_index(cls.index, index_path)\n",
    "        kwargs[\"df\"].reset_index(drop=True).to_csv(df_path, index=False)\n",
    "\n",
    "\n",
    "class reset_index_cb(Callback):\n",
    "    \"A `Callback` to reset the index.\"\n",
    "\n",
    "    def after_batch(self, cls, **kwargs):\n",
    "        cls.index.reset()\n",
    "        if self.verbose and cls.verbose:\n",
    "            msg.info(f\"Index Size Post Reset: {cls.index.ntotal}\")\n",
    "        cls.udf_kwargs[\"index\"] = cls.index\n",
    "        cls.udf = partial(cls.udf, **cls.udf_kwargs)\n",
    "\n",
    "\n",
    "class IndexCreator(Mapper):\n",
    "    \"Creates indexes from embeddings.\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index=None,\n",
    "        index_dim=3,  # The dimension of the index.\n",
    "        index_folder=\"indexes\",  # The folder to write the index to.\n",
    "        ems_col=\"embedding\",  # The column to use to create the index.\n",
    "        udf=df_to_index,  # The function to use to create the index.\n",
    "        cbs=[write_index_cb, reset_index_cb],  # The `Callback`s to use.\n",
    "        block_counter=0,  # The starting block counter.\n",
    "        verbose=True,  # Whether to print out information.\n",
    "        udf_verbose=False,  # Whether to print out information in the udf.\n",
    "        udf_kwargs={},  # Additional kwargs to pass to the udf.\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.index_folder = index_folder\n",
    "        if index is None:\n",
    "            self.index = create_index(index_dim)\n",
    "        else:\n",
    "            self.index = index\n",
    "        udf_kwargs[\"index\"] = self.index\n",
    "        udf_kwargs[\"ems_col\"] = ems_col\n",
    "        udf_kwargs[\"verbose\"] = udf_verbose\n",
    "        self.verbose = verbose\n",
    "        cbs = [block_counter_cb(block_counter)] + cbs\n",
    "        super().__init__(**locals_to_params(locals()))\n",
    "\n",
    "\n",
    "def create_indexes_setup(\n",
    "    task_id,\n",
    "    ems_folder,\n",
    "    index_folder=\"indexes\",\n",
    "    local_index_folder=\"/media/hamza/data2/faiss_data/saved_indexes\",\n",
    "    ems_col=\"embedding\",\n",
    "    num_blocks=1,\n",
    "    block_size=40000,\n",
    "    block_counter=0,\n",
    "    verbose=True,\n",
    "    *args,\n",
    "    **kwargs,\n",
    "):\n",
    "    task_folder = f\"/tmp/{task_id}\"\n",
    "    t1 = time()\n",
    "    ems_folder, _ = handle_input_path(ems_folder, local_path=task_folder)\n",
    "    em_files = sorted(\n",
    "        get_files(ems_folder, extensions=[\".json\"], make_str=True),\n",
    "        key=lambda x: int(Path(x).stem.split(\"_\")[-1]),\n",
    "    )\n",
    "\n",
    "    t2 = time()\n",
    "    msg.info(\n",
    "        f\"Embeddings download time: {t2-t1:.2f} seconds.\",\n",
    "        spaced=True,\n",
    "        show=verbose,\n",
    "    )\n",
    "    df = pd.DataFrame({ems_col: em_files})\n",
    "    msg.info(f\"Embeddings DF created of length: {len(df)}\", spaced=True, show=verbose)\n",
    "\n",
    "    index_folder, index_bucket = get_local_path(index_folder, local_path=task_folder)\n",
    "    local_index_folder = Path(local_index_folder) / Path(index_folder).name\n",
    "    if local_index_folder.exists():\n",
    "        msg.info(f\"Local Index Folder Exists: {local_index_folder}\", spaced=True, show=verbose)\n",
    "        bucket_size = len(get_files(local_index_folder, extensions=[\".faiss\"]))\n",
    "    else:\n",
    "        bucket_size = max(bucket_count(index_bucket), 0) // 2\n",
    "    block_counter += bucket_size\n",
    "    if num_blocks is not None:\n",
    "        block_size = len(df) // num_blocks\n",
    "    msg.info(f\"Bucket Size: {bucket_size}\", spaced=True, show=verbose)\n",
    "    msg.info(f\"Block Size: {block_size}\", spaced=True, show=verbose)\n",
    "    msg.info(f\"Block Counter: {block_counter}\", spaced=True, show=verbose)\n",
    "\n",
    "    return (\n",
    "        df,\n",
    "        block_size,\n",
    "        block_counter,\n",
    "        task_folder,\n",
    "        index_folder,\n",
    "        index_bucket,\n",
    "        local_index_folder,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_indexes_iter(\n",
    "    ems_folder=\"embeddings\",  # The folder containing the embeddings.\n",
    "    ems_col=\"embedding\",  # The column to use to create the index.\n",
    "    index_dim=768,  # The dimension of the index.\n",
    "    index_folder=\"indexes\",  # The folder to write the index to.\n",
    "    local_index_folder=\"/media/hamza/data2/faiss_data/saved_indexes\",  # The local folder to write the index to.\n",
    "    num_blocks=1,  # The number of blocks to make.\n",
    "    block_size=40000,  # The size of each block. Will be ignored if `num_blocks` is not None.\n",
    "    block_counter=0,  # The starting block counter.\n",
    "    verbose=True,  # Whether to print out information.\n",
    "    udf_verbose=False,  # Whether to print out information in the udf.\n",
    "    udf_kwargs={},  # Additional kwargs to pass to the udf.\n",
    "    task_id=gen_random_string(16),  # The task id to use.\n",
    "    *args,\n",
    "    **kwargs,\n",
    "):\n",
    "    t1 = time()\n",
    "    (\n",
    "        df,\n",
    "        block_size,\n",
    "        block_counter,\n",
    "        task_folder,\n",
    "        index_folder,\n",
    "        index_bucket,\n",
    "        local_index_folder,\n",
    "    ) = create_indexes_setup(\n",
    "        task_id=task_id,\n",
    "        ems_folder=ems_folder,\n",
    "        index_folder=index_folder,\n",
    "        local_index_folder=local_index_folder,\n",
    "        ems_col=ems_col,\n",
    "        num_blocks=num_blocks,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    t2 = time()\n",
    "    msg.info(f\"Setup time: {t2-t1:.2f} seconds.\", spaced=True, show=verbose)\n",
    "\n",
    "    for i in range(0, len(df), block_size):\n",
    "        cls_kwargs = dict(\n",
    "            index_dim=index_dim,\n",
    "            index_folder=index_folder,\n",
    "            ems_col=ems_col,\n",
    "            block_counter=block_counter,\n",
    "            verbose=verbose,\n",
    "            udf_verbose=udf_verbose,\n",
    "            udf_kwargs=udf_kwargs,\n",
    "        )\n",
    "        fn_kwargs = dict(\n",
    "            task_folder=task_folder,\n",
    "            index_folder=index_folder,\n",
    "            index_bucket=index_bucket,\n",
    "            local_index_folder=local_index_folder,\n",
    "        )\n",
    "        block_counter += 1\n",
    "        yield dict(df=df.iloc[i : i + block_size], fn_kwargs=fn_kwargs, cls_kwargs=cls_kwargs)\n",
    "\n",
    "\n",
    "def create_index_(\n",
    "    data_dict,\n",
    "    **kwargs,\n",
    "):\n",
    "    df = data_dict[\"df\"]\n",
    "    cls_kwargs = data_dict[\"cls_kwargs\"]\n",
    "    m = IndexCreator(**cls_kwargs)\n",
    "    m(df)\n",
    "    return data_dict[\"fn_kwargs\"]\n",
    "\n",
    "\n",
    "def create_indexes_combine(res_list, *args, **kwargs):\n",
    "    task_folder = res_list[0][\"task_folder\"]\n",
    "    index_folder = res_list[0][\"index_folder\"]\n",
    "    index_bucket = res_list[0][\"index_bucket\"]\n",
    "    local_index_folder = res_list[0][\"local_index_folder\"]\n",
    "    shutil.copytree(index_folder, local_index_folder, dirs_exist_ok=True)\n",
    "    bucket_up(index_folder, index_bucket, only_new=True)\n",
    "    shutil.rmtree(task_folder, ignore_errors=True)\n",
    "    return {\"index_folder\": str(index_bucket), \"local_index_folder\": str(local_index_folder)}\n",
    "\n",
    "\n",
    "def create_indexes(\n",
    "    ems_folder=\"embeddings\",  # The folder containing the embeddings.\n",
    "    ems_col=\"embedding\",  # The column to use to create the index.\n",
    "    index_dim=768,  # The dimension of the index.\n",
    "    index_folder=\"indexes\",  # The folder to write the index to.\n",
    "    local_index_folder=\"/media/hamza/data2/faiss_data/saved_indexes\",  # The local folder to write the index to.\n",
    "    num_blocks=1,  # The number of blocks to make.\n",
    "    block_size=40000,  # The size of each block. Will be ignored if `num_blocks` is not None.\n",
    "    block_counter=0,  # The starting block counter.\n",
    "    verbose=True,  # Whether to print out information.\n",
    "    udf_verbose=False,  # Whether to print out information in the udf.\n",
    "    udf_kwargs={},  # Additional kwargs to pass to the udf.\n",
    "    task_id=gen_random_string(16),  # The task id to use.\n",
    "    *args,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"Function to create indexes from embeddings.\"\n",
    "    t_1 = time()\n",
    "    try:\n",
    "        t1 = time()\n",
    "        iterator = create_indexes_iter(**locals_to_params(locals()))\n",
    "        t2 = time()\n",
    "        msg.info(f\"Iterator creation time: {t2-t1:.2f} seconds.\", spaced=True, show=verbose)\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"Error creating iterator: {e}\", spaced=True, show=verbose)\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        t1 = time()\n",
    "        create_res = [create_index_(data_dict) for data_dict in iterator]\n",
    "        t2 = time()\n",
    "        msg.info(f\"Index creation time: {t2-t1:.2f} seconds.\", spaced=True, show=verbose)\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"Error creating indexes: {e}\", spaced=True, show=verbose)\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        t1 = time()\n",
    "        combine_res = create_indexes_combine(create_res)\n",
    "        t2 = time()\n",
    "        msg.info(f\"Index combination time: {t2-t1:.2f} seconds.\", spaced=True, show=verbose)\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"Error combining indexes: {e}\", spaced=True, show=verbose)\n",
    "        raise e\n",
    "    t_2 = time()\n",
    "    msg.good(f\"Total creation time: {t_2-t_1:.2f} seconds.\", spaced=True, show=verbose)\n",
    "    return combine_res\n",
    "\n",
    "\n",
    "def search_indexes_setup(\n",
    "    task_id,\n",
    "    ems,\n",
    "    index_folder,\n",
    "    local_index_folder=\"/media/hamza/data2/faiss_data/saved_indexes\",\n",
    "    verbose=True,\n",
    "):\n",
    "    task_folder = f\"/tmp/{task_id}\"\n",
    "    # if local_index_folder is None:\n",
    "    #     index_folder, _ = handle_input_path(\n",
    "    #         index_folder, local_path=local_index_folder, task_id=task_id\n",
    "    #     )\n",
    "    # else:\n",
    "    pre_index_folder, _ = get_local_path(index_folder, local_path=local_index_folder)\n",
    "    if os.path.exists(pre_index_folder):\n",
    "        msg.info(f\"Cached Index Folder: {pre_index_folder}\", spaced=True, show=verbose)\n",
    "        index_folder = pre_index_folder\n",
    "    else:\n",
    "        index_folder, _ = handle_input_path(\n",
    "            index_folder, local_path=local_index_folder, task_id=task_id\n",
    "        )\n",
    "    bucket_dl(ems, task_folder)\n",
    "    ems_file = get_files(task_folder, extensions=[\".json\"])[0]\n",
    "    with open(ems_file) as f:\n",
    "        ems = json.load(f)[\"embedding\"]\n",
    "    indexes = sorted(\n",
    "        get_files(index_folder, extensions=[\".faiss\"]),\n",
    "        key=lambda x: int(x.stem.split(\"_\")[0]),\n",
    "    )\n",
    "    if not os.path.exists(index_folder) or len(indexes) == 0:\n",
    "        raise Exception(\n",
    "            f\"No indexes found in '{index_folder}' folder. Please create indexes first.\"\n",
    "        )\n",
    "    indexes = sorted(\n",
    "        get_files(index_folder, extensions=[\".faiss\"]),\n",
    "        key=lambda x: int(x.stem.split(\"_\")[0]),\n",
    "    )\n",
    "    return ems, indexes, index_folder, task_folder\n",
    "\n",
    "\n",
    "def search_indexes_iter(\n",
    "    ems,  # The embedding to search. Can be pre-loaded or a path to a json file.\n",
    "    index_folder,  # The remote folder containing the indexes.\n",
    "    local_index_folder=\"/media/hamza/data2/faiss_data/saved_indexes\",  # Not required if `index_folder` is local.\n",
    "    k=2,  # The number of nearest neighbors to return.\n",
    "    verbose=True,  # Whether to print out information.\n",
    "    task_id=gen_random_string(16),  # The task id to use.\n",
    "    *args,\n",
    "    **kwargs,\n",
    "):\n",
    "    ems, indexes, index_folder, task_folder = search_indexes_setup(\n",
    "        task_id, ems, index_folder, local_index_folder, verbose\n",
    "    )\n",
    "    for index in indexes:\n",
    "        yield {\n",
    "            \"qdf\": pd.DataFrame({\"index\": [index], \"embedding\": [ems]}),\n",
    "            \"k\": k,\n",
    "            \"index_folder\": index_folder,\n",
    "            \"task_folder\": task_folder,\n",
    "            \"verbose\": verbose,\n",
    "        }\n",
    "\n",
    "\n",
    "def search_index_(data_dict):\n",
    "    qdf = data_dict[\"qdf\"]\n",
    "    k = data_dict[\"k\"]\n",
    "    verbose = data_dict[\"verbose\"]\n",
    "    qdf = qdf.apply(lambda x: df_index_search(x, k=k, verbose=verbose), axis=1)\n",
    "    data_dict[\"qdf\"] = qdf\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def search_indexes_combine(res_list, *args, **kwargs):\n",
    "    r0 = res_list[0]\n",
    "    k = r0[\"k\"]\n",
    "    verbose = r0[\"verbose\"]\n",
    "    index_folder = r0[\"index_folder\"]\n",
    "    task_folder = r0[\"task_folder\"]\n",
    "    qdf = pd.concat([d[\"qdf\"] for d in res_list]).reset_index(drop=True)\n",
    "    res = index_heap(qdf, k=k, verbose=verbose, with_offset=True)\n",
    "    dfs = sorted(get_files(index_folder, extensions=[\".csv\"]), key=lambda x: int(x.stem))\n",
    "    df = pd.concat([pd.read_csv(df) for df in dfs]).reset_index(drop=True)\n",
    "    res[\"meta_data\"] = df.iloc[res[\"ids\"][0]].to_dict(orient=\"records\")\n",
    "    shutil.rmtree(task_folder)\n",
    "    return res\n",
    "\n",
    "\n",
    "def search_indexes(\n",
    "    ems,  # The embedding to search. Can be pre-loaded or a path to a json file.\n",
    "    index_folder,  # The remote folder containing the indexes.\n",
    "    local_index_folder=\"/media/hamza/data2/faiss_data/saved_indexes\",  # Not required if `index_folder` is local.\n",
    "    k=2,  # The number of nearest neighbors to return.\n",
    "    verbose=True,  # Whether to print out information.\n",
    "    task_id=gen_random_string(16),  # The task id to use.\n",
    "):\n",
    "    \"Function to search an embedding against indexes.\"\n",
    "\n",
    "    t_1 = time()\n",
    "    try:\n",
    "        t1 = time()\n",
    "        iterator = search_indexes_iter(**locals_to_params(locals()))\n",
    "        t2 = time()\n",
    "        msg.info(f\"Iterator creation time: {t2-t1:.2f} seconds.\", spaced=True, show=verbose)\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"Error creating iterator: {e}\", spaced=True, show=verbose)\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        t1 = time()\n",
    "        create_res = [search_index_(data_dict) for data_dict in iterator]\n",
    "        t2 = time()\n",
    "        msg.info(f\"Index creation time: {t2-t1:.2f} seconds.\", spaced=True, show=verbose)\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"Error creating indexes: {e}\", spaced=True, show=verbose)\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        t1 = time()\n",
    "        combine_res = search_indexes_combine(create_res)\n",
    "        t2 = time()\n",
    "        msg.info(f\"Search combination time: {t2-t1:.2f} seconds.\", spaced=True, show=verbose)\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"Error combining searches: {e}\", spaced=True, show=verbose)\n",
    "        raise e\n",
    "    t_2 = time()\n",
    "    msg.good(f\"Total search time: {t_2-t_1:.2f} seconds.\", spaced=True, show=verbose)\n",
    "    return combine_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "# ray.init()\n",
    "# ray.data.DataContext.get_current().execution_options.verbose_progress = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "np.random.seed(42)\n",
    "data_path = Path(\"/media/hamza/data2/faiss_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | eval: false\n",
    "\n",
    "# data_path = Path(\"\")\n",
    "# ems_folder = data_path / \"ems\"\n",
    "# index_folder = data_path / \"indexes\"\n",
    "# num_ems = 50\n",
    "# block_size = 10\n",
    "# ems_dim = 768\n",
    "# random_ems(num_ems=num_ems, ems_dim=ems_dim, ems_folder=ems_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "bucket = \"gs://gcsfuse-talentnet-dev\"\n",
    "\n",
    "ems_folder = f\"{bucket}/ems_1\"\n",
    "index_folder = f\"{bucket}/indexes_1\"\n",
    "local_index_folder = \"/media/hamza/data2/faiss_data/saved_indexes\"\n",
    "ems_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;4mℹ Deleting gs://gcsfuse-talentnet-dev/indexes_1.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "bucket_del(index_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;4mℹ Iterator creation time: 0.00 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Downloading gs://gcsfuse-talentnet-dev/ems_1 to\n",
      "/tmp/229c58383dd54f61/ems_1.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_1.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_10.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_11.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_12.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_13.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_15.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_14.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_16.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_2.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_4.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_9.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_5.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_7.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_3.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_6.json...\n",
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_8.json...\n",
      "- [16/16 files][267.4 KiB/267.4 KiB] 100% Done                                  \n",
      "Operation completed over 16 objects/267.4 KiB.                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;4mℹ Embeddings download time: 2.11 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Embeddings DF created of length: 16\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Local Index Folder Exists:\n",
      "/media/hamza/data2/faiss_data/saved_indexes/indexes_1\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Bucket Size: 4\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Block Size: 16\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Block Counter: 4\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Setup time: 2.11 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ DF BATCH SIZE: 16\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Index creation time: 2.12 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Uploading /tmp/229c58383dd54f61/indexes_1 to\n",
      "gs://gcsfuse-talentnet-dev/indexes_1.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/229c58383dd54f61/indexes_1/5_16.faiss [Content-Type=application/octet-stream]...\n",
      "Copying file:///tmp/229c58383dd54f61/indexes_1/5.csv [Content-Type=text/csv]... \n",
      "/ [1/2 files][ 49.3 KiB/ 49.3 KiB]  99% Done                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;4mℹ Index combination time: 2.16 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ Total creation time: 4.28 seconds.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- [2/2 files][ 49.3 KiB/ 49.3 KiB] 100% Done                                    \n",
      "Operation completed over 2 objects/49.3 KiB.                                     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index_folder': 'gs://gcsfuse-talentnet-dev/indexes_1',\n",
       " 'local_index_folder': Path('/media/hamza/data2/faiss_data/saved_indexes/indexes_1')}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "create_res = create_indexes(\n",
    "    ems_folder=ems_folder,\n",
    "    index_folder=index_folder,\n",
    "    local_index_folder=local_index_folder,\n",
    "    index_dim=ems_dim,\n",
    "    verbose=True,\n",
    "    num_blocks=1,\n",
    ")\n",
    "\n",
    "create_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;4mℹ Iterator creation time: 0.00 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Cached Index Folder:\n",
      "/media/hamza/data2/faiss_data/saved_indexes/indexes_1\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Downloading\n",
      "gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_10.json\n",
      "to /tmp/d8f359817d4c4f1e.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://gcsfuse-talentnet-dev/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_10.json...\n",
      "/ [0/1 files][    0.0 B/ 16.7 KiB]   0% Done                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Index Col:\n",
      "/media/hamza/data2/faiss_data/saved_indexes/indexes_1/1_4.faiss\u001b[0m\n",
      "\u001b[38;5;4mℹ Index Size: 4\u001b[0m\n",
      "\u001b[38;5;4mℹ Ems Shape: (1, 768)\u001b[0m\n",
      "\u001b[38;5;2m✔ IDs: [[ 1  3  0  2 -1]], Distances: [[1.1570426e+00 1.2090571e+00\n",
      "1.2641366e+00 1.5258880e+00 3.4028235e+38]]\u001b[0m\n",
      "\u001b[38;5;4mℹ Index Col:\n",
      "/media/hamza/data2/faiss_data/saved_indexes/indexes_1/2_4.faiss\u001b[0m\n",
      "\u001b[38;5;4mℹ Index Size: 4\u001b[0m\n",
      "\u001b[38;5;4mℹ Ems Shape: (1, 768)\u001b[0m\n",
      "\u001b[38;5;2m✔ IDs: [[ 3  0  1  2 -1]], Distances: [[1.2113628e+00 1.2323084e+00\n",
      "1.2558432e+00 1.2952166e+00 3.4028235e+38]]\u001b[0m\n",
      "\u001b[38;5;4mℹ Index Col:\n",
      "/media/hamza/data2/faiss_data/saved_indexes/indexes_1/3_4.faiss\u001b[0m\n",
      "\u001b[38;5;4mℹ Index Size: 4\u001b[0m\n",
      "\u001b[38;5;4mℹ Ems Shape: (1, 768)\u001b[0m\n",
      "\u001b[38;5;2m✔ IDs: [[ 1  3  0  2 -1]], Distances: [[0.0000000e+00 9.2401117e-01\n",
      "1.0372934e+00 1.2498804e+00 3.4028235e+38]]\u001b[0m\n",
      "\u001b[38;5;4mℹ Index Col:\n",
      "/media/hamza/data2/faiss_data/saved_indexes/indexes_1/4_4.faiss\u001b[0m\n",
      "\u001b[38;5;4mℹ Index Size: 4\u001b[0m\n",
      "\u001b[38;5;4mℹ Ems Shape: (1, 768)\u001b[0m\n",
      "\u001b[38;5;2m✔ IDs: [[ 1  2  0  3 -1]], Distances: [[1.1016233e+00 1.1049132e+00\n",
      "1.2239318e+00 1.4562318e+00 3.4028235e+38]]\u001b[0m\n",
      "\u001b[38;5;4mℹ Index Col:\n",
      "/media/hamza/data2/faiss_data/saved_indexes/indexes_1/5_16.faiss\u001b[0m\n",
      "\u001b[38;5;4mℹ Index Size: 16\u001b[0m\n",
      "\u001b[38;5;4mℹ Ems Shape: (1, 768)\u001b[0m\n",
      "\u001b[38;5;2m✔ IDs: [[ 9 11  8 13 14]], Distances: [[0.        0.9240112 1.0372934\n",
      "1.1016233 1.1049132]]\u001b[0m\n",
      "\n",
      "\u001b[38;5;4mℹ Index creation time: 1.87 seconds.\u001b[0m\n",
      "\n",
      "\u001b[38;5;4mℹ Adding Result: [[1.1570426e+00 1.2090571e+00 1.2641366e+00\n",
      "1.5258880e+00 3.4028235e+38]], [[ 1  3  0  2 -1]]\u001b[0m\n",
      "\u001b[38;5;2m✔ Added Result: [[1.1570426e+00 1.2090571e+00 1.2641366e+00\n",
      "1.5258880e+00 3.4028235e+38]], [[ 1  3  0  2 -1]]\u001b[0m\n",
      "\u001b[38;5;4mℹ Adding Result: [[1.2113628e+00 1.2323084e+00 1.2558432e+00\n",
      "1.2952166e+00 3.4028235e+38]], [[7 4 5 6 3]]\u001b[0m\n",
      "\u001b[38;5;2m✔ Added Result: [[1.2113628e+00 1.2323084e+00 1.2558432e+00\n",
      "1.2952166e+00 3.4028235e+38]], [[7 4 5 6 3]]\u001b[0m\n",
      "\u001b[38;5;4mℹ Adding Result: [[0.0000000e+00 9.2401117e-01 1.0372934e+00\n",
      "1.2498804e+00 3.4028235e+38]], [[ 9 11  8 10  7]]\u001b[0m\n",
      "\u001b[38;5;2m✔ Added Result: [[0.0000000e+00 9.2401117e-01 1.0372934e+00\n",
      "1.2498804e+00 3.4028235e+38]], [[ 9 11  8 10  7]]\u001b[0m\n",
      "\u001b[38;5;4mℹ Adding Result: [[1.1016233e+00 1.1049132e+00 1.2239318e+00\n",
      "1.4562318e+00 3.4028235e+38]], [[13 14 12 15 11]]\u001b[0m\n",
      "\u001b[38;5;2m✔ Added Result: [[1.1016233e+00 1.1049132e+00 1.2239318e+00\n",
      "1.4562318e+00 3.4028235e+38]], [[13 14 12 15 11]]\u001b[0m\n",
      "\u001b[38;5;4mℹ Adding Result: [[0.        0.9240112 1.0372934 1.1016233 1.1049132]],\n",
      "[[25 27 24 29 30]]\u001b[0m\n",
      "\u001b[38;5;2m✔ Added Result: [[0.        0.9240112 1.0372934 1.1016233 1.1049132]],\n",
      "[[25 27 24 29 30]]\u001b[0m\n",
      "\n",
      "\u001b[38;5;4mℹ Search combination time: 0.01 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ Total search time: 1.88 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Final Results:\n",
      "\tDistances: [[0.0, 0.0, 0.9240111708641052, 0.9240111708641052, 1.0372934341430664]]\n",
      "\tIDs: [[9, 25, 11, 27, 8]]\n",
      "\tMeta Data:\n",
      "\t\t{'embedding': '/tmp/229c58383dd54f61/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_10.json'}\n",
      "\t\t{'embedding': '/tmp/229c58383dd54f61/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_10.json'}\n",
      "\t\t{'embedding': '/tmp/229c58383dd54f61/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_12.json'}\n",
      "\t\t{'embedding': '/tmp/229c58383dd54f61/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_12.json'}\n",
      "\t\t{'embedding': '/tmp/229c58383dd54f61/ems_1/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_9.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ [1/1 files][ 16.7 KiB/ 16.7 KiB] 100% Done                                    \n",
      "Operation completed over 1 objects/16.7 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "qems = f\"{ems_folder}/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_10.json\"\n",
    "res = search_indexes(\n",
    "    qems,\n",
    "    index_folder=index_folder,\n",
    "    local_index_folder=local_index_folder,\n",
    "    k=5,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f'\\n\\nFinal Results:\\n\\tDistances: {res[\"distances\"]}\\n\\tIDs: {res[\"ids\"]}')\n",
    "print(\"\\tMeta Data:\")\n",
    "for m in res[\"meta_data\"]:\n",
    "    print(f\"\\t\\t{m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | eval: false\n",
    "\n",
    "# indexes_iter = create_indexes_iter(\n",
    "#     ems_folder=ems_folder,\n",
    "#     index_folder=index_folder,\n",
    "#     index_dim=ems_dim,\n",
    "#     verbose=True,\n",
    "#     block_size=4,\n",
    "# )\n",
    "\n",
    "# res = [create_index_(d) for d in indexes_iter]\n",
    "\n",
    "# indexes_up(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| eval: false\n",
    "\n",
    "# qems = f\"{ems_folder}/resumes-4e2cdbeb-1e20-45ff-bded-a0a510350167_10.json\"\n",
    "# data_dict = dict(ems=qems, index_folder=index_folder, k=5, verbose=True)\n",
    "# search_iter = search_indexes_iter(data_dict=data_dict)\n",
    "# res = [search_index_(d) for d in search_iter]\n",
    "# res = combine_searches(res, data_dict)\n",
    "\n",
    "# print(f'\\n\\nFinal Results:\\n\\tDistances: {res[\"distances\"]}\\n\\tIDs: {res[\"ids\"]}')\n",
    "# print(\"\\tMeta Data:\")\n",
    "# for m in res[\"meta_data\"]:\n",
    "#     print(f\"\\t\\t{m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "shutil.rmtree(local_index_folder, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
